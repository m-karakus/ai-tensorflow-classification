{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/metin2/sentiment-analysis?scriptVersionId=82515310\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Model Creation \n---","metadata":{}},{"cell_type":"code","source":"!pip install opendatasets\n!pip install nltk\n!pip install TurkishStemmer\n!pip install sklearn\n!pip install keras\n!pip install tensorflow\n!pip install googletrans==4.0.0-rc1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import opendatasets as od\nod.download(\"https://www.kaggle.com/metin2/sentimentanalysis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ssl\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\nimport codecs\nimport os\nimport re \nimport pandas as pd\nimport seaborn as sn\nimport numpy as np\nimport tensorflow as tf\nimport codecs\nimport string \nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:48:20.490762Z","iopub.execute_input":"2021-12-16T12:48:20.492022Z","iopub.status.idle":"2021-12-16T12:48:20.507317Z","shell.execute_reply.started":"2021-12-16T12:48:20.491973Z","shell.execute_reply":"2021-12-16T12:48:20.506349Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"#df = pd.read_csv('sentimentAnalysis.csv')\ndf = pd.read_csv('../input/sentimentanalysis/sentimentAnalysis.csv')\ndf[[\"TranslatedText\",\"SentimentAnalysis\"]].head(10)\n\n#when SentimentAnalysis=0 then 'Ignore'\n#when SentimentAnalysis=1 then 'Negative'\n#when SentimentAnalysis=2 then 'Neutral'\n#when SentimentAnalysis=3 then 'Positive' end as SentimentAnalysis","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:48:29.186518Z","iopub.execute_input":"2021-12-16T12:48:29.18681Z","iopub.status.idle":"2021-12-16T12:48:31.42269Z","shell.execute_reply.started":"2021-12-16T12:48:29.186778Z","shell.execute_reply":"2021-12-16T12:48:31.421461Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                      TranslatedText  SentimentAnalysis\n0                             Use omo get immaculate                  2\n1                 2 times wear and bury the closet üòÇ                  2\n2  Several Different Several Different Boylece Ol...                  2\n3                          Low degree, use little ..                  2\n4                   Legend I don't believe something                  2\n5        I think the most useful and useful rinso.üôÇüëç                  3\n6  Don't need to have a lot of reversed and have ...                  2\n7                      Health thanks to your uniform                  2\n8                  HEALTHY INFORMATION HEALTH HEALTH                  0\n9              A correct degree and a good detergent                  2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TranslatedText</th>\n      <th>SentimentAnalysis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Use omo get immaculate</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2 times wear and bury the closet üòÇ</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Several Different Several Different Boylece Ol...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Low degree, use little ..</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Legend I don't believe something</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I think the most useful and useful rinso.üôÇüëç</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Don't need to have a lot of reversed and have ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Health thanks to your uniform</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>HEALTHY INFORMATION HEALTH HEALTH</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A correct degree and a good detergent</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def textCleaning(text):\n    # lower\n    text = text.lower()\n\n    # remove hyperlinks\n    text = re.sub(r\"<.*?a>\", \"\", text)\n\n    # remove html\n    text = re.sub(r\"<[^>]+>\", \"\", text)\n\n    # remove urls\n    text = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"\", text)\n    \n    # encoding the text - cleaning the emoji\n    text_encode = text.encode(encoding=\"utf-8\", errors=\"ignore\")\n    # text_encode = text.encode(encoding=\"latin5\", errors=\"ignore\")\n\n    # decoding the text\n    text_decode = text_encode.decode(\"ascii\", errors=\"ignore\")\n    # text_decode = text_encode.decode(\"latin5\", errors=\"replace_with_space\")\n    text = text_decode\n    \n    # Remove ticks and the next character\n    text = re.sub(r\"\\'\\w+\", \"\", text)\n\n    # Remove ticks and the next character\n    text = re.sub(r\"\\'\\w+\", \"\", text)\n\n    # cleaning the text to remove extra whitespace \n    text = re.sub(r'\\s{2,}', \" \", text)\n\n    # # removing mentions \n    text = re.sub(r\"@\\S+\", \"\", text)\n\n    # # Remove Hashtags\n    text = re.sub(r\"#\\S+\", \"\", text)\n    \n    # remove market tickers\n    text = re.sub(r\"\\$\", \"\", text)\n\n    # remove punctuation\n    stringPunct= '''!\"$%&'()*+,-./:;<=>?[\\]^_`{|}~'''\n    text = text.translate(str.maketrans('', '',stringPunct))\n\n    # Remove numbers\n    text = re.sub(r'\\w*\\d+\\w*', '', text)\n\n    # Tokenizer\n    tt = TweetTokenizer()\n    words = tt.tokenize(text)\n\n    #hastags = []\n    #[hastags.append(token) for token in words if token.startswith(\"#\") ]\n\n    #mentions = []\n    #[mentions.append(token) for token in words if token.startswith(\"@\") ]\n\n    words = [word for word in words if word not in stopwords.words('english')]\n    #the stemmer requires a language parameter\n    snow_stemmer = SnowballStemmer(language='english')\n    \n    #stem's of each word\n    stem_words = []\n    for w in words:\n        x = snow_stemmer.stem(w)\n        stem_words.append(x)\n\n    return (stem_words)\n\n#print(stopwords.words('english'))\n#textCleaning(\"I always tell my female clients to chat/text for a day or so then move it to\")","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:48:37.859516Z","iopub.execute_input":"2021-12-16T12:48:37.860069Z","iopub.status.idle":"2021-12-16T12:48:37.873451Z","shell.execute_reply.started":"2021-12-16T12:48:37.860019Z","shell.execute_reply":"2021-12-16T12:48:37.872494Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X=df[\"TranslatedText\"].tolist()\ny=df[\"SentimentAnalysis\"].tolist()\n#train, test = train_test_split(df, test_size=0.2, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n\nText_X_train, Text_X_test, Text_y_train, Text_y_test = X_train, X_test, y_train, y_test \n\ny_train = np.array(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:48:40.046876Z","iopub.execute_input":"2021-12-16T12:48:40.047159Z","iopub.status.idle":"2021-12-16T12:48:40.32288Z","shell.execute_reply.started":"2021-12-16T12:48:40.047129Z","shell.execute_reply":"2021-12-16T12:48:40.321798Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(X_train[0])\nprint(y[0])\n\nprint(len(X_train))\nprint(len(y_train))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-16T12:48:43.343931Z","iopub.execute_input":"2021-12-16T12:48:43.344217Z","iopub.status.idle":"2021-12-16T12:48:43.350773Z","shell.execute_reply.started":"2021-12-16T12:48:43.344188Z","shell.execute_reply":"2021-12-16T12:48:43.349722Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Bi times a week stir and take care and take care of the eeeen beauty as well.I never do the paint ‚ù§ and I immediately wash my hair off the sea.\n2\n183635\n183635\n","output_type":"stream"}]},{"cell_type":"code","source":"max_features = 7000\n\ntokenized = X_train\n#df1[\"AnswerText\"].tolist()\n\nvectorizer = TfidfVectorizer(analyzer='word',\n                        tokenizer=textCleaning,\n                        vocabulary=None,\n                        # preprocessor=dummy_fun,\n                        lowercase = False,\n                        use_idf = True,\n                        token_pattern=None,\n                        ngram_range=(1,3),\n                        max_df=0.9, min_df=2,\n                        max_features=max_features\n                    )\ntfs = vectorizer.fit_transform(tokenized).todense()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:48:46.343039Z","iopub.execute_input":"2021-12-16T12:48:46.343582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = vectorizer.vocabulary_\n# print(vocabulary)\n# print(vectorizer.get_feature_names_out())\nprint(len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:58:38.304844Z","iopub.execute_input":"2021-12-16T11:58:38.305098Z","iopub.status.idle":"2021-12-16T11:58:38.31142Z","shell.execute_reply.started":"2021-12-16T11:58:38.305067Z","shell.execute_reply":"2021-12-16T11:58:38.310298Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"7000\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tfs.shape)\nprint(y_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = vectorizer.transform(X_test).todense()\ny_test = np.array(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:58:38.327177Z","iopub.execute_input":"2021-12-16T11:58:38.327426Z","iopub.status.idle":"2021-12-16T12:01:06.329813Z","shell.execute_reply.started":"2021-12-16T11:58:38.327395Z","shell.execute_reply":"2021-12-16T12:01:06.329199Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#initial model\nmodel = keras.Sequential([\n    keras.layers.Dense(128, input_shape=(7000,),activation='relu'),\n    keras.layers.Dense(64),\n    keras.layers.Dense(16),\n    keras.layers.Dense(4,activation='sigmoid')\n])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compiling of the model\nmodel.compile(\n    optimizer='adam',\n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nmodel.fit(tfs, y_train, epochs=5)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-16T12:01:07.554998Z","iopub.execute_input":"2021-12-16T12:01:07.555977Z","iopub.status.idle":"2021-12-16T12:04:04.606942Z","shell.execute_reply.started":"2021-12-16T12:01:07.555939Z","shell.execute_reply":"2021-12-16T12:04:04.605629Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2021-12-16 12:01:23.308551: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n5739/5739 [==============================] - 34s 6ms/step - loss: 0.6389 - accuracy: 0.7645\nEpoch 2/5\n5739/5739 [==============================] - 31s 5ms/step - loss: 0.5629 - accuracy: 0.7864\nEpoch 3/5\n5739/5739 [==============================] - 33s 6ms/step - loss: 0.5100 - accuracy: 0.8024\nEpoch 4/5\n5739/5739 [==============================] - 33s 6ms/step - loss: 0.4420 - accuracy: 0.8261\nEpoch 5/5\n5739/5739 [==============================] - 30s 5ms/step - loss: 0.3728 - accuracy: 0.8537\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fa8d074fc10>"},"metadata":{}}]},{"cell_type":"code","source":"model.save(\"sentimentAnalysis.h5\")\nimport pickle\nfilename=\"vectorizer\"\nwith open(filename, 'wb') as fp:\n    pickle.dump(vectorizer, fp, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T20:03:41.210656Z","iopub.execute_input":"2021-12-15T20:03:41.211Z","iopub.status.idle":"2021-12-15T20:03:41.807822Z","shell.execute_reply.started":"2021-12-15T20:03:41.210963Z","shell.execute_reply":"2021-12-15T20:03:41.806705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:04:04.608928Z","iopub.execute_input":"2021-12-16T12:04:04.6092Z","iopub.status.idle":"2021-12-16T12:04:13.952718Z","shell.execute_reply.started":"2021-12-16T12:04:04.609163Z","shell.execute_reply":"2021-12-16T12:04:13.95187Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"2460/2460 [==============================] - 6s 2ms/step - loss: 0.7800 - accuracy: 0.7681\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[0.7800027132034302, 0.7680715322494507]"},"metadata":{}}]},{"cell_type":"code","source":"y_predicted = model.predict(X_test)\ny_predicted[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:04:13.953931Z","iopub.execute_input":"2021-12-16T12:04:13.954144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Text: \", Text_X_test[0])\nprint(\"label: \",np.argmax(y_predicted[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Draw a graph\ny_predicted = model.predict(X_test)\ny_predicted_labels = [np.argmax(i) for i in y_predicted]\ncm = tf.math.confusion_matrix(labels=y_test, predictions= y_predicted_labels )\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = [\"I'm sleeping regularly 8 hours for health and I'm considering very balanced nutrition.I also use regular Oalrak food supplies\",\"it is ok, love it\"]\nx = vectorizer.transform(text).todense()\nnp.argmax(model.predict(x)[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Part\n---","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nimport pickle\n\nm = keras.models.load_model(\"sentimentAnalysis.h5\")\n\nwith open(\"vectorizer\", 'rb') as pfile:\n    v = pickle.load(pfile)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = [\"I'm sleeping regularly 8 hours for health and I'm considering very balanced nutrition.I also use regular Oalrak food supplies\",\"it is ok, love it\"]\nx = v.transform(text).todense()\nnp.argmax(m.predict(x)[0])","metadata":{},"execution_count":null,"outputs":[]}]}